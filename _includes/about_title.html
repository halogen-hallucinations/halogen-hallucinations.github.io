    <!-- About Section -->
    <section id="about" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
              <br/><br/>
                <h2>HALoGEN</h2>
            <h4> <i>Fantastic LLM Hallucinations and Where To Find Them</i></h4>
                <a href="https://lasharavichander.github.io/">Abhilasha Ravichander*</a>, <a href="https://shrustighela.com/">Shrusti Ghela*</a>, <a href="https://dwadden.github.io/">David Wadden</a>, <a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a>
              </div>
        </div>
    </section>

<br/>


        
<div class="container">
    
    <div class="row">
        <div class="lead">
            
            <p>
Despite their impressive ability to generate high-quality and fluent text, generative large language models (LLMs) also produce <b>hallucinations</b>: fabricated statements that contain false information, or that deviate from provided context (for example when a model is asked to summarize a document, but inserts new facts in the summary that were never in the provided document).</br></br> 
                AI hallucinations pose serious risks: they can propagate false information, erode user trust, and render these systems unsuitable for critical applications. Understanding how often these hallucinations occur and what causes them remains a fundamental challenge in developing trustworthy AI systems. </p>
            </br>
            <img src="../img/gpt4_hall_truncated.png" alt="Centered Image" style="display: block; margin: auto; max-width: 40%; max-height: 100vh;">
            <center><figcaption style="font-size: 12px;">A hallucinated historical narrative produced by GPT-4.</figcaption></center></br></br>

            
            <p>We release üî¶HALoGEN, a benchmark to <b>measure LLM hallucinations</b> which consists of:</br>
            üîé <b>10,923</b> prompts for generative models spanning nine domains including programming, scientific attribution, and summarization.</br>
            üîß <b>Automatic high-precision verifiers</b> for each use case that decompose LLM generations into atomic units, and verify each unit against a high-quality knowledge source to identify hallucinations.</br></br>
            

        Building on this framework we provide:</br>
        
        ü§ñ <b>150,000</b> generations from <b>14 large language models</b> with automated factuality annotations.</br>
        ‚≠ê A novel <b>hallucination taxonomy</b> for identifying causes of model hallucination based on their <span style="color: maroon;">origin in training data</span>.</br>

<center>
<table style="width:80%; border-coll apse: collapse; font-family: Arial, sans-serif;">
      <tr>
    <th style="border: 1px solid black; background-color: #f2f2f2; padding: 8px; text-align: left;">
      Hallucination Type
    </th>
    <th style="border: 1px solid black; background-color: #f2f2f2; padding: 8px; text-align: left;">
      Description
    </th>
  </tr>
  <tr>
    <td  style="border: 1px solid black; padding: 8px;">Type A</td>
    <td  style="border: 1px solid black; padding: 8px;">The correct fact was present in the pretraining data.</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 8px;">Type B</td>
    <td style="border: 1px solid black; padding: 8px;">An incorrect fact was in the pretraining data, or the fact is taken out of context
i.e. the fact appeared within a specific setting in a document in the training data, but when taken in isolation, it loses its original meaning.</td>   </tr>

  <tr>
    <td style="border: 1px solid black; padding: 8px;">Type C</td>
    <td style="border: 1px solid black; padding: 8px;"> Neither a correct nor an incorrect fact was present in the pretraining data, and the model over-generalized when making predictions.</td>
  </tr>
</table>
</center>
                    
                

                <img src="../img/halogen_Teaser.png" alt="Centered Image" style="display: block; margin: auto; max-width: 100%; max-height: 100vh;">

                You can find more information in our paper <a href="https://arxiv.org/abs/2102.08345">here</a>.
    
        
               
            </p>

        <p> 

        </div>

    </div>
</div>
<br/>
