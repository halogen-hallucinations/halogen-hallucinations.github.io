    <!-- About Section -->
    <section id="about" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
              <br/><br/>
                <h2>HALoGEN</h2>
            <h4> <i>Fantastic LLM Hallucinations and Where To Find Them</i></h4>
              </div>
        </div>
    </section>

<br/>
<div class="container">
    
    <div class="row">
        <div class="lead">

            <p>
Despite their impressive ability to generate high-quality and fluent text, generative large language models (LLMs) also produce hallucinations: statements that are misaligned with established world knowledge or provided input context. </p>
            </br>
            <p>We release:
                <ul>
                <li> 10,923 prompts for generative models spanning nine domains including programming, scientific attribution, and summarization</li>
                    
                    <li>Automatic high-precision verifiers for each use case that decompose LLM generations into atomic units, and verify each unit against a high-quality knowledge source</li>
                    <li>150,000 generations from 14 language models.</li>
                    <li> A novel error taxonomy for causes of model hallucination based on their origin in pretraining data.</li>
                    
                </ul>
        
                You can find all the details in our paper <a href="https://arxiv.org/abs/2102.08345">here</a>.
    
        
               
            </p>

        <p> 

        </div>

    </div>
</div>
<br/>
