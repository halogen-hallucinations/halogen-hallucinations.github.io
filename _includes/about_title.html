    <!-- About Section -->
    <section id="about" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
              <br/><br/>
                <h2>HALoGEN</h2>
            <h4> <i>Fantastic LLM Hallucinations and Where To Find Them</i></h4>
              </div>
        </div>
    </section>

<br/>


        
<div class="container">
    
    <div class="row">
        <div class="lead">
            
            <p>
Despite their impressive ability to generate high-quality and fluent text, generative large language models (LLMs) also produce hallucinations: statements that are misaligned with established world knowledge or provided input context. </p>
            </br>
            <p>We release:
                <ul>
                <li> <b>10,923</b> prompts for generative models spanning nine domains including programming, scientific attribution, and summarization.</li>
                    
                    <li><b>Automatic high-precision verifiers</b> for each use case that decompose LLM generations into atomic units, and verify each unit against a high-quality knowledge source.</li>
                    <li><b>150,000</b> generations from <b>14 large language models</b>.</li>
                    <li> A novel <b>hallucination taxonomy</b> for identifying causes of model hallucination based on their origin in pretraining data.</li>
                    
                </ul>

                <img src="../img/halogen_Teaser.png" alt="Centered Image" style="display: block; margin: auto; max-width: 100%; max-height: 1000vh;">

                You can find all the details in our paper <a href="https://arxiv.org/abs/2102.08345">here</a>.
    
        
               
            </p>

        <p> 

        </div>

    </div>
</div>
<br/>
